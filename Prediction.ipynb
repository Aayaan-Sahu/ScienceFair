{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math                                           # Various math functions\n",
    "import numpy as np                                    # Special array objects useful for machine learning\n",
    "import pandas as pd                                   # Versatile dataframe structures\n",
    "import tensorflow as tf                               # Machine learning library\n",
    "import matplotlib.pyplot as plt                       # Useful for making graphs and plots\n",
    "from sklearn.preprocessing import MinMaxScaler        # Scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data.csv\") # Store the data from the csv file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DAPR</th>\n",
       "      <th>DASF</th>\n",
       "      <th>MDPR</th>\n",
       "      <th>MDSF</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>...</th>\n",
       "      <th>TOBS</th>\n",
       "      <th>WT01</th>\n",
       "      <th>WT03</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT05</th>\n",
       "      <th>WT06</th>\n",
       "      <th>WT08</th>\n",
       "      <th>WT09</th>\n",
       "      <th>WT11</th>\n",
       "      <th>WT14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26092</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26093</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26094</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26095</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26096</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26097 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           STATION                   NAME        DATE  DAPR  DASF  MDPR  MDSF  \\\n",
       "0      USC00045933  MOUNT HAMILTON, CA US  1948-07-01   NaN   NaN   NaN   NaN   \n",
       "1      USC00045933  MOUNT HAMILTON, CA US  1948-07-02   NaN   NaN   NaN   NaN   \n",
       "2      USC00045933  MOUNT HAMILTON, CA US  1948-07-03   NaN   NaN   NaN   NaN   \n",
       "3      USC00045933  MOUNT HAMILTON, CA US  1948-07-04   NaN   NaN   NaN   NaN   \n",
       "4      USC00045933  MOUNT HAMILTON, CA US  1948-07-05   NaN   NaN   NaN   NaN   \n",
       "...            ...                    ...         ...   ...   ...   ...   ...   \n",
       "26092  USC00045933  MOUNT HAMILTON, CA US  2020-11-13   NaN   NaN   NaN   NaN   \n",
       "26093  USC00045933  MOUNT HAMILTON, CA US  2020-11-14   NaN   NaN   NaN   NaN   \n",
       "26094  USC00045933  MOUNT HAMILTON, CA US  2020-11-15   NaN   NaN   NaN   NaN   \n",
       "26095  USC00045933  MOUNT HAMILTON, CA US  2020-11-16   NaN   NaN   NaN   NaN   \n",
       "26096  USC00045933  MOUNT HAMILTON, CA US  2020-11-17   NaN   NaN   NaN   NaN   \n",
       "\n",
       "       PRCP  SNOW  SNWD  ...  TOBS  WT01  WT03  WT04  WT05  WT06  WT08  WT09  \\\n",
       "0      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4      0.00   0.0   0.0  ...   NaN   1.0   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...     ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "26092  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26093  0.31   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26094  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26095  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26096  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "       WT11  WT14  \n",
       "0       NaN   NaN  \n",
       "1       NaN   NaN  \n",
       "2       NaN   NaN  \n",
       "3       NaN   NaN  \n",
       "4       NaN   NaN  \n",
       "...     ...   ...  \n",
       "26092   NaN   NaN  \n",
       "26093   NaN   NaN  \n",
       "26094   NaN   NaN  \n",
       "26095   NaN   NaN  \n",
       "26096   NaN   NaN  \n",
       "\n",
       "[26097 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATION\n",
      "NAME\n",
      "DATE\n",
      "DAPR\n",
      "DASF\n",
      "MDPR\n",
      "MDSF\n",
      "PRCP\n",
      "SNOW\n",
      "SNWD\n",
      "TMAX\n",
      "TMIN\n",
      "TOBS\n",
      "WT01\n",
      "WT03\n",
      "WT04\n",
      "WT05\n",
      "WT06\n",
      "WT08\n",
      "WT09\n",
      "WT11\n",
      "WT14\n"
     ]
    }
   ],
   "source": [
    "for column in df:\n",
    "    print(column) # A list of all the columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = []\n",
    "\n",
    "for x in range(len(df[\"TMAX\"])):  # Find the average of maximum and minimum temperatures and put them in the temperatures list\n",
    "    max = df.loc[x, \"TMAX\"]\n",
    "    min = df.loc[x, \"TMIN\"]\n",
    "    average = (max + min) / 2\n",
    "    temperatures.append(average)\n",
    "    \n",
    "temperatures = np.array(temperatures) # Convert into a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [x for x in temperatures if math.isnan(x) == False] # Remove all NaN objects\n",
    "temperatures = temperatures[:25170]\n",
    "temperatures = np.array(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74. , 69.5, 62. , ..., 48.5, 43. , 47.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For better accuracy with the model, data should be scaled. Here the data is scaled between values of -1 and 1. The model can make faster and better predictions with these smaller and easier numbers which will result in faster training and better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "temperatures = scaler.fit_transform(temperatures.reshape(-1, 1)) # Scale all the data from values from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57055215],\n",
       "       [ 0.4601227 ],\n",
       "       [ 0.27607362],\n",
       "       ...,\n",
       "       [-0.05521472],\n",
       "       [-0.19018405],\n",
       "       [-0.0797546 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures # The data is now scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data and test data\n",
    "test_data_length = 20\n",
    "\n",
    "# Split the data into training data and testing data. The length of testing data will be 20\n",
    "train_data = temperatures[:len(temperatures) - test_data_length]\n",
    "test_data = temperatures[-test_data_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features and labels are an essential part of machine learning. The features are the data that is being passed to the model such as weather data, and the label is the actual value and the result of those labels. In this case the features are going to be the past 5 days' temperatures and the label is going to be the temperature of the next day. With these values, the model can make sense of the relationship of the previous five days and the prediction of the day after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training data is the data that the model uses to recognize the relationships between the features and the labels. In our case we pass in the train_data which contains of the train_x and the train_y arrays. The train_x contains the features and the train_y contains the labels. The model can see what the actual data and it can learn based on actual events so that it can make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = 5\n",
    "\n",
    "def create_inout_sequences(input_data, tw):\n",
    "  features = []\n",
    "  labels = []\n",
    "  L = len(input_data)\n",
    "  for i in range(L - tw):\n",
    "    train_seq = input_data[i : i + tw]\n",
    "    train_label = input_data[i + tw : i + tw + 1]\n",
    "    features.append((train_seq))\n",
    "    labels.append(train_label)\n",
    "\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = create_inout_sequences(train_data, train_window)\n",
    "train_x, train_y = np.array(train_x), np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = create_inout_sequences(test_data, train_window)\n",
    "test_x, test_y = np.array(test_x), np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape([train_x.shape[0], train_x.shape[1], 1]) # Resphape the training data to make it 3 Dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.57055215],\n",
       "        [ 0.4601227 ],\n",
       "        [ 0.27607362],\n",
       "        [ 0.11656442],\n",
       "        [-0.05521472]],\n",
       "\n",
       "       [[ 0.4601227 ],\n",
       "        [ 0.27607362],\n",
       "        [ 0.11656442],\n",
       "        [-0.05521472],\n",
       "        [ 0.04294479]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.04294479]],\n",
       "\n",
       "       [[0.10429448]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential                     # The type of model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout    # The layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = Sequential()\n",
    "\n",
    "regression.add(LSTM(units=256, activation='tanh', return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "regression.add(Dropout(0.1))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "# regression.add(Dropout(0.2))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh'))#, return_sequences=True))\n",
    "# regression.add(Dropout(0.3))\n",
    "\n",
    "# regression.add(LSTM(units=64, activation='tanh'))\n",
    "# regression.add(Dropout(0.1))\n",
    "\n",
    "regression.add(Dense(units=1))\n",
    "\n",
    "# Tweak the unit values and the number of LSTM and Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 5, 256)            264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 128)            197120    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 856,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regression.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "393/393 [==============================] - 15s 39ms/step - loss: 0.0295 - val_loss: 0.0405\n",
      "Epoch 2/150\n",
      "393/393 [==============================] - 12s 30ms/step - loss: 0.0162 - val_loss: 0.0416\n",
      "Epoch 3/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0162 - val_loss: 0.0460\n",
      "Epoch 4/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0162 - val_loss: 0.0437\n",
      "Epoch 5/150\n",
      "393/393 [==============================] - 12s 30ms/step - loss: 0.0162 - val_loss: 0.0489\n",
      "Epoch 6/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0161 - val_loss: 0.0448\n",
      "Epoch 7/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0160 - val_loss: 0.0478\n",
      "Epoch 8/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0160 - val_loss: 0.0485\n",
      "Epoch 9/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0161 - val_loss: 0.0520\n",
      "Epoch 10/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0161 - val_loss: 0.0474\n",
      "Epoch 11/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0160 - val_loss: 0.0489\n",
      "Epoch 12/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0472\n",
      "Epoch 13/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0477\n",
      "Epoch 14/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0489\n",
      "Epoch 15/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0480\n",
      "Epoch 16/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0504\n",
      "Epoch 17/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0159 - val_loss: 0.0485\n",
      "Epoch 18/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0469\n",
      "Epoch 19/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0516\n",
      "Epoch 20/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0158 - val_loss: 0.0451\n",
      "Epoch 21/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0486\n",
      "Epoch 22/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0458\n",
      "Epoch 23/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0476\n",
      "Epoch 24/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0520\n",
      "Epoch 25/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0470\n",
      "Epoch 26/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0450\n",
      "Epoch 27/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0493\n",
      "Epoch 28/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0490\n",
      "Epoch 29/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0458\n",
      "Epoch 30/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0158 - val_loss: 0.0467\n",
      "Epoch 31/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0158 - val_loss: 0.0481\n",
      "Epoch 32/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0501\n",
      "Epoch 33/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0465\n",
      "Epoch 34/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0158 - val_loss: 0.0466\n",
      "Epoch 35/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0511\n",
      "Epoch 36/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0474\n",
      "Epoch 37/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0474\n",
      "Epoch 38/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0466\n",
      "Epoch 39/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0502\n",
      "Epoch 40/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0503\n",
      "Epoch 41/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0158 - val_loss: 0.0466\n",
      "Epoch 42/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0475\n",
      "Epoch 43/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0503\n",
      "Epoch 44/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0157 - val_loss: 0.0485\n",
      "Epoch 45/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0438\n",
      "Epoch 46/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0464\n",
      "Epoch 47/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0157 - val_loss: 0.0512\n",
      "Epoch 48/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0490\n",
      "Epoch 49/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0157 - val_loss: 0.0474\n",
      "Epoch 50/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0522\n",
      "Epoch 51/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0157 - val_loss: 0.0509\n",
      "Epoch 52/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0486\n",
      "Epoch 53/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0472\n",
      "Epoch 54/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0156 - val_loss: 0.0508\n",
      "Epoch 55/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0157 - val_loss: 0.0440\n",
      "Epoch 56/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0471\n",
      "Epoch 57/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0157 - val_loss: 0.0458\n",
      "Epoch 58/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0486\n",
      "Epoch 59/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0474\n",
      "Epoch 60/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0449\n",
      "Epoch 61/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0157 - val_loss: 0.0465\n",
      "Epoch 62/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0156 - val_loss: 0.0479\n",
      "Epoch 63/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0156 - val_loss: 0.0490\n",
      "Epoch 64/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0463\n",
      "Epoch 65/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0462\n",
      "Epoch 66/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0454\n",
      "Epoch 67/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0456\n",
      "Epoch 68/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0494\n",
      "Epoch 69/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0460\n",
      "Epoch 70/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0156 - val_loss: 0.0505\n",
      "Epoch 71/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0480\n",
      "Epoch 72/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0479\n",
      "Epoch 73/150\n",
      " 85/393 [=====>........................] - ETA: 8s - loss: 0.0162"
     ]
    }
   ],
   "source": [
    "regression.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=150, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regression.predict(train_x[:3])\n",
    "predictions = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_predictions = train_y[:3]\n",
    "actual_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted = []\n",
    "reformatted.append(actual_predictions[0][0])\n",
    "reformatted.append(actual_predictions[1][0])\n",
    "reformatted.append(actual_predictions[2][0])\n",
    "reformatted = np.array(reformatted).astype(\"float\")\n",
    "\n",
    "reformatted = scaler.inverse_transform(reformatted)\n",
    "reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Predictions vs Real\")\n",
    "plt.ylabel(\"Actual temperature\")\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(scaler.inverse_transform(temperatures[-100:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPredictions = regression.predict(test_x[-10:])\n",
    "plotPredictions = scaler.inverse_transform(plotPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(90, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.reshape(z, (10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Predictions vs Real\")\n",
    "plt.ylabel(\"Actual temperature\")\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(scaler.inverse_transform(temperatures[-100:]))\n",
    "plt.plot(z, plotPredictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [[84], [88], [89], [87], [80], [83], [77], [80], [82], [80], [72], [72], [77], [78], [78]]\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array(points)\n",
    "points = scaler.fit_transform(points)\n",
    "x, y = create_inout_sequences(points, 5)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "predictions = regression.predict(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(temperatures[23:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions():\n",
    "    points = []\n",
    "    iterations = int(input('How many days of weather will you be providing (Minimum:5): '))\n",
    "    while iterations < 5:\n",
    "        print(\"You will need to enter at least 5.\")\n",
    "        iterations = int(input('How many days of weather will you be providing (Minimum:5): '))\n",
    "    if iterations == 5:\n",
    "        points = [[]]\n",
    "        for i in range(5):\n",
    "            data = float(input(f'Enter day number {i + 1}: '))\n",
    "            points.append([[[data]]])\n",
    "            points[0].append([data])\n",
    "        points = np.array(points)\n",
    "        \n",
    "        return points\n",
    "#         points = scaler.fit_transform(points)\n",
    "#         return scaler.inverse_transform(regression.predict(points))\n",
    "    else:\n",
    "        for i in range(iterations):\n",
    "            data = float(input(f'Enter day number {i + 1}: '))\n",
    "            points.append([data])\n",
    "        points = np.array(points)\n",
    "        points = scaler.fit_transform(points)\n",
    "        x, y = create_inout_sequences(points, 5)\n",
    "        x, y = np.array(x), np.array(y)\n",
    "        \n",
    "        tempPrediction = regression.predict(x)\n",
    "        tempPrediction = tempPrediction.reshape(tempPrediction.shape[0], tempPrediction.shape[1])\n",
    "        \n",
    "        return scaler.inverse_transform(tempPrediction)\n",
    "        \n",
    "#     if iterations < 6:\n",
    "#         print(\"You will need to enter six.\")\n",
    "#         iterations = 6\n",
    "#     for i in range(iterations):\n",
    "#         data = float(input(f'Enter day number {i + 1}: '))\n",
    "#         points.append([data])\n",
    "        \n",
    "#     points = np.array(points)\n",
    "#     points = scaler.fit_transform(points)\n",
    "#     x, y = create_inout_sequences(points, 5)\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)\n",
    "    \n",
    "#     return scaler.inverse_transform(regression.predict(x)), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
