{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math                                           # Various math functions\n",
    "import numpy as np                                    # Special array objects useful for machine learning\n",
    "import pandas as pd                                   # Versatile dataframe structures\n",
    "import tensorflow as tf                               # Machine learning library\n",
    "import matplotlib.pyplot as plt                       # Useful for making graphs and plots\n",
    "from sklearn.preprocessing import MinMaxScaler        # Scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data.csv\") # Store the data from the csv file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DAPR</th>\n",
       "      <th>DASF</th>\n",
       "      <th>MDPR</th>\n",
       "      <th>MDSF</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>...</th>\n",
       "      <th>TOBS</th>\n",
       "      <th>WT01</th>\n",
       "      <th>WT03</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT05</th>\n",
       "      <th>WT06</th>\n",
       "      <th>WT08</th>\n",
       "      <th>WT09</th>\n",
       "      <th>WT11</th>\n",
       "      <th>WT14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>1948-07-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26092</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26093</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26094</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26095</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26096</th>\n",
       "      <td>USC00045933</td>\n",
       "      <td>MOUNT HAMILTON, CA US</td>\n",
       "      <td>2020-11-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26097 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           STATION                   NAME        DATE  DAPR  DASF  MDPR  MDSF  \\\n",
       "0      USC00045933  MOUNT HAMILTON, CA US  1948-07-01   NaN   NaN   NaN   NaN   \n",
       "1      USC00045933  MOUNT HAMILTON, CA US  1948-07-02   NaN   NaN   NaN   NaN   \n",
       "2      USC00045933  MOUNT HAMILTON, CA US  1948-07-03   NaN   NaN   NaN   NaN   \n",
       "3      USC00045933  MOUNT HAMILTON, CA US  1948-07-04   NaN   NaN   NaN   NaN   \n",
       "4      USC00045933  MOUNT HAMILTON, CA US  1948-07-05   NaN   NaN   NaN   NaN   \n",
       "...            ...                    ...         ...   ...   ...   ...   ...   \n",
       "26092  USC00045933  MOUNT HAMILTON, CA US  2020-11-13   NaN   NaN   NaN   NaN   \n",
       "26093  USC00045933  MOUNT HAMILTON, CA US  2020-11-14   NaN   NaN   NaN   NaN   \n",
       "26094  USC00045933  MOUNT HAMILTON, CA US  2020-11-15   NaN   NaN   NaN   NaN   \n",
       "26095  USC00045933  MOUNT HAMILTON, CA US  2020-11-16   NaN   NaN   NaN   NaN   \n",
       "26096  USC00045933  MOUNT HAMILTON, CA US  2020-11-17   NaN   NaN   NaN   NaN   \n",
       "\n",
       "       PRCP  SNOW  SNWD  ...  TOBS  WT01  WT03  WT04  WT05  WT06  WT08  WT09  \\\n",
       "0      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3      0.00   0.0   0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4      0.00   0.0   0.0  ...   NaN   1.0   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...     ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "26092  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26093  0.31   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26094  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26095  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "26096  0.00   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "       WT11  WT14  \n",
       "0       NaN   NaN  \n",
       "1       NaN   NaN  \n",
       "2       NaN   NaN  \n",
       "3       NaN   NaN  \n",
       "4       NaN   NaN  \n",
       "...     ...   ...  \n",
       "26092   NaN   NaN  \n",
       "26093   NaN   NaN  \n",
       "26094   NaN   NaN  \n",
       "26095   NaN   NaN  \n",
       "26096   NaN   NaN  \n",
       "\n",
       "[26097 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATION\n",
      "NAME\n",
      "DATE\n",
      "DAPR\n",
      "DASF\n",
      "MDPR\n",
      "MDSF\n",
      "PRCP\n",
      "SNOW\n",
      "SNWD\n",
      "TMAX\n",
      "TMIN\n",
      "TOBS\n",
      "WT01\n",
      "WT03\n",
      "WT04\n",
      "WT05\n",
      "WT06\n",
      "WT08\n",
      "WT09\n",
      "WT11\n",
      "WT14\n"
     ]
    }
   ],
   "source": [
    "for column in df:\n",
    "    print(column) # A list of all the columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = []\n",
    "\n",
    "for x in range(len(df[\"TMAX\"])):  # Find the average of maximum and minimum temperatures and put them in the temperatures list\n",
    "    max = df.loc[x, \"TMAX\"]\n",
    "    min = df.loc[x, \"TMIN\"]\n",
    "    average = (max + min) / 2\n",
    "    temperatures.append(average)\n",
    "    \n",
    "temperatures = np.array(temperatures) # Convert into a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [x for x in temperatures if math.isnan(x) == False] # Remove all NaN objects\n",
    "temperatures = temperatures[:25170]\n",
    "temperatures = np.array(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74. , 69.5, 62. , ..., 48.5, 43. , 47.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For better accuracy with the model, data should be scaled. Here the data is scaled between values of -1 and 1. The model can make faster and better predictions with these smaller and easier numbers which will result in faster training and better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "temperatures = scaler.fit_transform(temperatures.reshape(-1, 1)) # Scale all the data from values from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57055215],\n",
       "       [ 0.4601227 ],\n",
       "       [ 0.27607362],\n",
       "       ...,\n",
       "       [-0.05521472],\n",
       "       [-0.19018405],\n",
       "       [-0.0797546 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures # The data is now scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data and test data\n",
    "test_data_length = 20\n",
    "\n",
    "# Split the data into training data and testing data. The length of testing data will be 20\n",
    "train_data = temperatures[:len(temperatures) - test_data_length]\n",
    "test_data = temperatures[-test_data_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features and labels are an essential part of machine learning. The features are the data that is being passed to the model such as weather data, and the label is the actual value and the result of those labels. In this case the features are going to be the past 5 days' temperatures and the label is going to be the temperature of the next day. With these values, the model can make sense of the relationship of the previous five days and the prediction of the day after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training data is the data that the model uses to recognize the relationships between the features and the labels. In our case we pass in the train_data which contains of the train_x and the train_y arrays. The train_x contains the features and the train_y contains the labels. The model can see what the actual data and it can learn based on actual events so that it can make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = 5\n",
    "\n",
    "def create_inout_sequences(input_data, tw):\n",
    "  features = []\n",
    "  labels = []\n",
    "  L = len(input_data)\n",
    "  for i in range(L - tw):\n",
    "    train_seq = input_data[i : i + tw]\n",
    "    train_label = input_data[i + tw : i + tw + 1]\n",
    "    features.append((train_seq))\n",
    "    labels.append(train_label)\n",
    "\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = create_inout_sequences(train_data, train_window)\n",
    "train_x, train_y = np.array(train_x), np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = create_inout_sequences(test_data, train_window)\n",
    "test_x, test_y = np.array(test_x), np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape([train_x.shape[0], train_x.shape[1], 1]) # Resphape the training data to make it 3 Dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.57055215],\n",
       "        [ 0.4601227 ],\n",
       "        [ 0.27607362],\n",
       "        [ 0.11656442],\n",
       "        [-0.05521472]],\n",
       "\n",
       "       [[ 0.4601227 ],\n",
       "        [ 0.27607362],\n",
       "        [ 0.11656442],\n",
       "        [-0.05521472],\n",
       "        [ 0.04294479]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:2] # shows the first two elements of the train_x array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.04294479]],\n",
       "\n",
       "       [[0.10429448]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:2] # shows the first two elements of the train_y array\n",
    "            # these elements correspond to the first elements in the train_x array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential                     # The type of model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout    # The layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = Sequential()\n",
    "\n",
    "# Tweak the unit values and the number of LSTM and Dropout layers\n",
    "\n",
    "regression.add(LSTM(units=256, activation='tanh', return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "regression.add(Dropout(0.1))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "\n",
    "regression.add(LSTM(units=128, activation='tanh'))\n",
    "\n",
    "regression.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 5, 256)            264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 128)            197120    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 856,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regression.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "393/393 [==============================] - 22s 36ms/step - loss: 0.0493 - val_loss: 0.0409\n",
      "Epoch 2/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0169 - val_loss: 0.0410\n",
      "Epoch 3/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0161 - val_loss: 0.0432\n",
      "Epoch 4/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0163 - val_loss: 0.0421\n",
      "Epoch 5/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0438\n",
      "Epoch 6/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0475\n",
      "Epoch 7/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0162 - val_loss: 0.0474\n",
      "Epoch 8/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0163 - val_loss: 0.0483\n",
      "Epoch 9/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0160 - val_loss: 0.0463\n",
      "Epoch 10/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0426\n",
      "Epoch 11/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0160 - val_loss: 0.0500\n",
      "Epoch 12/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0464\n",
      "Epoch 13/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0157 - val_loss: 0.0502\n",
      "Epoch 14/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0157 - val_loss: 0.0549\n",
      "Epoch 15/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0159 - val_loss: 0.0470\n",
      "Epoch 16/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0160 - val_loss: 0.0502\n",
      "Epoch 17/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0161 - val_loss: 0.0461\n",
      "Epoch 18/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0160 - val_loss: 0.0487\n",
      "Epoch 19/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0496\n",
      "Epoch 20/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0453\n",
      "Epoch 21/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0502\n",
      "Epoch 22/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0495\n",
      "Epoch 23/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0508\n",
      "Epoch 24/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0474\n",
      "Epoch 25/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0474\n",
      "Epoch 26/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0462\n",
      "Epoch 27/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0465\n",
      "Epoch 28/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0460\n",
      "Epoch 29/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0461\n",
      "Epoch 30/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0491\n",
      "Epoch 31/150\n",
      "393/393 [==============================] - 12s 29ms/step - loss: 0.0158 - val_loss: 0.0459\n",
      "Epoch 32/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0449\n",
      "Epoch 33/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0453\n",
      "Epoch 34/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0483\n",
      "Epoch 35/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0155 - val_loss: 0.0493\n",
      "Epoch 36/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0156 - val_loss: 0.0476\n",
      "Epoch 37/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0161 - val_loss: 0.0489\n",
      "Epoch 38/150\n",
      "393/393 [==============================] - 12s 32ms/step - loss: 0.0158 - val_loss: 0.0492\n",
      "Epoch 39/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0487\n",
      "Epoch 40/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0472\n",
      "Epoch 41/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0482\n",
      "Epoch 42/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0159 - val_loss: 0.0475\n",
      "Epoch 43/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0468\n",
      "Epoch 44/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0473\n",
      "Epoch 45/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0158 - val_loss: 0.0486\n",
      "Epoch 46/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0476\n",
      "Epoch 47/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0470\n",
      "Epoch 48/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0469\n",
      "Epoch 49/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0507\n",
      "Epoch 50/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0498\n",
      "Epoch 51/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0465\n",
      "Epoch 52/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0158 - val_loss: 0.0464\n",
      "Epoch 53/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0474\n",
      "Epoch 54/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0477\n",
      "Epoch 55/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0478\n",
      "Epoch 56/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0158 - val_loss: 0.0483\n",
      "Epoch 57/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0158 - val_loss: 0.0475\n",
      "Epoch 58/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0156 - val_loss: 0.0468\n",
      "Epoch 59/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0156 - val_loss: 0.0476\n",
      "Epoch 60/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0157 - val_loss: 0.0478\n",
      "Epoch 61/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0509\n",
      "Epoch 62/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0158 - val_loss: 0.0457\n",
      "Epoch 63/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0524\n",
      "Epoch 64/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0158 - val_loss: 0.0455\n",
      "Epoch 65/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0443\n",
      "Epoch 66/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0510\n",
      "Epoch 67/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0447\n",
      "Epoch 68/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0157 - val_loss: 0.0496\n",
      "Epoch 69/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0153 - val_loss: 0.0457\n",
      "Epoch 70/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0156 - val_loss: 0.0464\n",
      "Epoch 71/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0157 - val_loss: 0.0495\n",
      "Epoch 72/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0155 - val_loss: 0.0488\n",
      "Epoch 73/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0155 - val_loss: 0.0478\n",
      "Epoch 74/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0158 - val_loss: 0.0459\n",
      "Epoch 75/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0476\n",
      "Epoch 76/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0450\n",
      "Epoch 77/150\n",
      "393/393 [==============================] - 12s 29ms/step - loss: 0.0157 - val_loss: 0.0476\n",
      "Epoch 78/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0159 - val_loss: 0.0491\n",
      "Epoch 79/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0156 - val_loss: 0.0457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0152 - val_loss: 0.0472\n",
      "Epoch 81/150\n",
      "393/393 [==============================] - 10s 25ms/step - loss: 0.0156 - val_loss: 0.0498\n",
      "Epoch 82/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0454\n",
      "Epoch 83/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0466\n",
      "Epoch 84/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0462\n",
      "Epoch 85/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0444\n",
      "Epoch 86/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0157 - val_loss: 0.0462\n",
      "Epoch 87/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0156 - val_loss: 0.0452\n",
      "Epoch 88/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0153 - val_loss: 0.0464\n",
      "Epoch 89/150\n",
      "393/393 [==============================] - 11s 27ms/step - loss: 0.0157 - val_loss: 0.0452\n",
      "Epoch 90/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0158 - val_loss: 0.0481\n",
      "Epoch 91/150\n",
      "393/393 [==============================] - 10s 27ms/step - loss: 0.0154 - val_loss: 0.0489\n",
      "Epoch 92/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0151 - val_loss: 0.0455\n",
      "Epoch 93/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0153 - val_loss: 0.0453\n",
      "Epoch 94/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0155 - val_loss: 0.0461\n",
      "Epoch 95/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0154 - val_loss: 0.0460\n",
      "Epoch 96/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0445\n",
      "Epoch 97/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0156 - val_loss: 0.0457\n",
      "Epoch 98/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0155 - val_loss: 0.0455\n",
      "Epoch 99/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0457\n",
      "Epoch 100/150\n",
      "393/393 [==============================] - 12s 30ms/step - loss: 0.0153 - val_loss: 0.0447\n",
      "Epoch 101/150\n",
      "393/393 [==============================] - 12s 30ms/step - loss: 0.0156 - val_loss: 0.0452\n",
      "Epoch 102/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0489\n",
      "Epoch 103/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0154 - val_loss: 0.0469\n",
      "Epoch 104/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0154 - val_loss: 0.0429\n",
      "Epoch 105/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0156 - val_loss: 0.0469\n",
      "Epoch 106/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0460\n",
      "Epoch 107/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0155 - val_loss: 0.0444\n",
      "Epoch 108/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0152 - val_loss: 0.0460\n",
      "Epoch 109/150\n",
      "393/393 [==============================] - 12s 30ms/step - loss: 0.0153 - val_loss: 0.0444\n",
      "Epoch 110/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0154 - val_loss: 0.0484\n",
      "Epoch 111/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0156 - val_loss: 0.0471\n",
      "Epoch 112/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0157 - val_loss: 0.0446\n",
      "Epoch 113/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0154 - val_loss: 0.0476\n",
      "Epoch 114/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0156 - val_loss: 0.0451\n",
      "Epoch 115/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0152 - val_loss: 0.0447\n",
      "Epoch 116/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0152 - val_loss: 0.0469\n",
      "Epoch 117/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0152 - val_loss: 0.0493\n",
      "Epoch 118/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0155 - val_loss: 0.0438\n",
      "Epoch 119/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0470\n",
      "Epoch 120/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0424\n",
      "Epoch 121/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0158 - val_loss: 0.0486\n",
      "Epoch 122/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0155 - val_loss: 0.0454\n",
      "Epoch 123/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0464\n",
      "Epoch 124/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0461\n",
      "Epoch 125/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0152 - val_loss: 0.0483\n",
      "Epoch 126/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0153 - val_loss: 0.0461\n",
      "Epoch 127/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0153 - val_loss: 0.0484\n",
      "Epoch 128/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0156 - val_loss: 0.0459\n",
      "Epoch 129/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0155 - val_loss: 0.0469\n",
      "Epoch 130/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0153 - val_loss: 0.0470\n",
      "Epoch 131/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0153 - val_loss: 0.0468\n",
      "Epoch 132/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0157 - val_loss: 0.0445\n",
      "Epoch 133/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0152 - val_loss: 0.0474\n",
      "Epoch 134/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0155 - val_loss: 0.0477\n",
      "Epoch 135/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0151 - val_loss: 0.0481\n",
      "Epoch 136/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0155 - val_loss: 0.0470\n",
      "Epoch 137/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0152 - val_loss: 0.0457\n",
      "Epoch 138/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0150 - val_loss: 0.0474\n",
      "Epoch 139/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0153 - val_loss: 0.0463\n",
      "Epoch 140/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0513\n",
      "Epoch 141/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0152 - val_loss: 0.0479\n",
      "Epoch 142/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0153 - val_loss: 0.0447\n",
      "Epoch 143/150\n",
      "393/393 [==============================] - 11s 29ms/step - loss: 0.0154 - val_loss: 0.0491\n",
      "Epoch 144/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0154 - val_loss: 0.0520\n",
      "Epoch 145/150\n",
      "393/393 [==============================] - 11s 28ms/step - loss: 0.0154 - val_loss: 0.0472\n",
      "Epoch 146/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0153 - val_loss: 0.0434\n",
      "Epoch 147/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0154 - val_loss: 0.0479\n",
      "Epoch 148/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0153 - val_loss: 0.0460\n",
      "Epoch 149/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0152 - val_loss: 0.0491\n",
      "Epoch 150/150\n",
      "393/393 [==============================] - 10s 26ms/step - loss: 0.0153 - val_loss: 0.0478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x116db6ae370>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=150, batch_size=64, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
